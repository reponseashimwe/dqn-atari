{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. âš™ï¸ Installation & Imports\n",
        "Run this cell first. It installs all dependencies and handles the necessary environment registration."
      ],
      "metadata": {
        "id": "JIsl5EEWRdAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6ab7u-6RUl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fc4513-06f1-4866-ce74-d1c20d7b4f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Running core imports.\n"
          ]
        }
      ],
      "source": [
        "# 1. INSTALLATION, REGISTRATION, AND IMPORTS\n",
        "\n",
        "# Install core DRL libraries\n",
        "!pip install stable-baselines3[extra] gymnasium[atari] ale-py -q\n",
        "# Install OpenCV and AutoROM (required dependencies for Atari wrappers)\n",
        "!pip install opencv-python autorom[accept-rom-license] -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import os\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import json\n",
        "\n",
        "# FIX: Explicitly register the ALE environments to avoid NamespaceNotFound\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "print(\"Setup complete. Running core imports.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ğŸ“ Drive Setup and Experiment Definitions\n",
        "This cells handles mounting the drive and defines the five hyperparameter sets, including the one that achieved your best result (Set 1)."
      ],
      "metadata": {
        "id": "OKaFhcQmRp4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive Mount\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Drive Configuration ---\n",
        "# Mount Drive first\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive'\n",
        "ASSIGNMENT_FOLDER = 'DQN_Breakout_Final_Submission'\n",
        "BASE_DEST_PATH = f'{DRIVE_PATH}/{ASSIGNMENT_FOLDER}/Experiments'\n",
        "os.makedirs(BASE_DEST_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6m4NoZGRiV9",
        "outputId": "fa3f0a3b-4deb-4b83-97c8-2dede56ceaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global DRL Configuration ---\n",
        "ENV_ID = \"BreakoutNoFrameskip-v4\"\n",
        "TOTAL_TIMESTEPS = 1_000_000\n",
        "N_ENVS = 4\n",
        "TEMP_MODEL_DIR = \"./temp_models/\"\n",
        "os.makedirs(TEMP_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "RESULTS_FILE = f'{BASE_DEST_PATH}/experiment_results.json'\n",
        "\n",
        "# Dictionary to store results for final comparison\n",
        "results_comparison = defaultdict(lambda: {'avg_reward': 0, 'params': {}})\n",
        "\n",
        "# --- Hyperparameter Definitions (5 Sets) ---\n",
        "HYPER_SETS = {\n",
        "    'Set_6_Even_Larger_Batch': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 256, # Pushing Set 5 further\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    },\n",
        "    'Set_7_Large_Batch_Slower_LR': {\n",
        "        'lr': 5e-5, # Slower learning rate (half of baseline)\n",
        "        'gamma': 0.99, 'batch_size': 128,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    },\n",
        "    'Set_8_Large_Batch_Long_Epsilon': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 128,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.3 # Longer decay (30%)\n",
        "    },\n",
        "    'Set_9_Large_Batch_Higher_Final_Eps': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 128,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.1, # Keep more exploration\n",
        "        'epsilon_decay': 0.1\n",
        "    },\n",
        "    'Set_10_Large_Batch_Faster_LR': {\n",
        "        'lr': 2e-4, # Faster learning rate (2x baseline)\n",
        "        'gamma': 0.99, 'batch_size': 128,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    }\n",
        "}\n",
        "print(\"Experiment definitions complete. Starting training runs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtbzPHshR6iy",
        "outputId": "1ecd4738-0199-4c31-8a0e-839846aa3adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment definitions complete. Starting training runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. ğŸ§ª Training, Evaluation, and Drive Saving Loop (Core Logic)\n",
        "This cell defines the functions and runs all five experiments, saving each to a distinct Drive folder."
      ],
      "metadata": {
        "id": "_wEntRpVSJGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment(env_id, n_envs):\n",
        "    env = make_atari_env(env_id, n_envs=n_envs, seed=np.random.randint(1000))\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent(model, env_id, n_episodes=5):\n",
        "    \"\"\"Runs evaluation episodes, PRINTS individual results, and returns average.\"\"\"\n",
        "    eval_env = setup_environment(env_id, n_envs=1)\n",
        "    episode_rewards = []\n",
        "\n",
        "    print(f\"\\nStarting evaluation ({n_episodes} episodes)...\")\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        obs = eval_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done_array, info = eval_env.step(action)\n",
        "            total_reward += reward[0]\n",
        "            done = done_array[0]\n",
        "        episode_rewards.append(total_reward)\n",
        "        print(f\"Episode {i+1} finished with reward: {total_reward:.2f}\")\n",
        "\n",
        "    eval_env.close()\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    print(f\"--> Average Reward over {n_episodes} episodes: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "def save_result_to_drive(name, reward, model_path):\n",
        "    \"\"\"Reads the existing JSON on Drive, updates it, and saves it back.\"\"\"\n",
        "    data = {}\n",
        "    # 1. Load existing data if file exists\n",
        "    if os.path.exists(RESULTS_FILE):\n",
        "        try:\n",
        "            with open(RESULTS_FILE, 'r') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            data = {} # Start fresh if file is corrupt\n",
        "\n",
        "    reward_value = float(reward)\n",
        "\n",
        "    # 2. Update with new result\n",
        "    data[name] = {\n",
        "        'avg_reward': reward_value,\n",
        "        'model_path': model_path\n",
        "    }\n",
        "\n",
        "    # 3. Write back to Drive\n",
        "    with open(RESULTS_FILE, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "    print(f\"ğŸ“ Result saved to persistent log: {RESULTS_FILE}\")\n",
        "\n",
        "def run_single_experiment(set_name):\n",
        "    \"\"\"Runs ONE experiment based on the set name key.\"\"\"\n",
        "    hyperparams = HYPER_SETS[set_name]\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"ğŸš€ STARTING EXPERIMENT: {set_name}\")\n",
        "    print(f\"Parameters: {hyperparams}\")\n",
        "    print(f\"=\"*60)\n",
        "\n",
        "    # Paths\n",
        "    LOG_DIR = os.path.join(TEMP_MODEL_DIR, f\"{set_name}_logs\")\n",
        "    TEMP_MODEL_PATH = os.path.join(TEMP_MODEL_DIR, f\"{set_name}.zip\")\n",
        "    DRIVE_DEST_FOLDER = os.path.join(BASE_DEST_PATH, set_name)\n",
        "    os.makedirs(DRIVE_DEST_FOLDER, exist_ok=True)\n",
        "\n",
        "    # Init Environment & Model\n",
        "    env = setup_environment(ENV_ID, N_ENVS)\n",
        "\n",
        "    # Callback\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq=200000 // N_ENVS, save_path=LOG_DIR, name_prefix=\"dqn_chkpt\"\n",
        "    )\n",
        "\n",
        "    model = DQN(\n",
        "        \"CnnPolicy\", env,\n",
        "        learning_rate=hyperparams['lr'], gamma=hyperparams['gamma'],\n",
        "        batch_size=hyperparams['batch_size'],\n",
        "        exploration_initial_eps=hyperparams['epsilon_start'],\n",
        "        exploration_final_eps=hyperparams['epsilon_end'],\n",
        "        exploration_fraction=hyperparams['epsilon_decay'],\n",
        "        buffer_size=50000,\n",
        "        train_freq=4,\n",
        "        verbose=0,\n",
        "        tensorboard_log=LOG_DIR\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"â³ Training started... (Wait for progress bar)\")\n",
        "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=checkpoint_callback, progress_bar=True)\n",
        "    model.save(TEMP_MODEL_PATH)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"\\nğŸ“Š Evaluating {set_name}...\")\n",
        "    avg_reward = evaluate_agent(model, ENV_ID, n_episodes=5)\n",
        "\n",
        "    # Save Artifacts to Drive\n",
        "    print(f\"\\nğŸ’¾ Copying artifacts to Drive Folder: {DRIVE_DEST_FOLDER}...\")\n",
        "    !cp -r \"$LOG_DIR\" \"$DRIVE_DEST_FOLDER/\"\n",
        "    !cp \"$TEMP_MODEL_PATH\" \"$DRIVE_DEST_FOLDER/\"\n",
        "\n",
        "    # --- AUTOMATION STEP: Save Score to JSON ---\n",
        "    final_drive_model_path = os.path.join(DRIVE_DEST_FOLDER, f\"{set_name}.zip\")\n",
        "    save_result_to_drive(set_name, avg_reward, final_drive_model_path)\n",
        "\n",
        "    # Cleanup\n",
        "    env.close()\n",
        "    del model\n",
        "    del env\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "setVfz02SOPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#â–¶ï¸ Execution Cells (Run One by One)"
      ],
      "metadata": {
        "id": "G74m3BV2GlvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Set 1 (Best Policy)"
      ],
      "metadata": {
        "id": "2agdb-PrGowQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_6_Even_Larger_Batch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "srO3lFHDSY3f",
        "outputId": "50512434-0f09-474d-ce8a-8aa321f9c656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m \u001b[32m999,976/1,000,000 \u001b[0m [ \u001b[33m0:44:22\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m386 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,976/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:44:22</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">386 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Evaluating Set_6_Even_Larger_Batch...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 14.00\n",
            "Episode 2 finished with reward: 4.00\n",
            "Episode 3 finished with reward: 13.00\n",
            "Episode 4 finished with reward: 3.00\n",
            "Episode 5 finished with reward: 4.00\n",
            "--> Average Reward over 5 episodes: 7.60\n",
            "\n",
            "ğŸ’¾ Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_6_Even_Larger_Batch...\n",
            "ğŸ“ Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 2 (High LR)"
      ],
      "metadata": {
        "id": "0-J0LNs3Gvtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_7_Large_Batch_Slower_LR')"
      ],
      "metadata": {
        "id": "u9X-fIr8Gy4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "e9cdf5e7-c431-48b7-a29c-2e1813a71285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m \u001b[32m999,988/1,000,000 \u001b[0m [ \u001b[33m0:37:21\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m446 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,988/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:37:21</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">446 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Evaluating Set_7_Large_Batch_Slower_LR...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 3.00\n",
            "Episode 2 finished with reward: 4.00\n",
            "Episode 3 finished with reward: 2.00\n",
            "Episode 4 finished with reward: 1.00\n",
            "Episode 5 finished with reward: 3.00\n",
            "--> Average Reward over 5 episodes: 2.60\n",
            "\n",
            "ğŸ’¾ Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_7_Large_Batch_Slower_LR...\n",
            "ğŸ“ Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 3 (Low Gamma)"
      ],
      "metadata": {
        "id": "IAi9OlL_Gzfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_8_Large_Batch_Long_Epsilon')"
      ],
      "metadata": {
        "id": "kJ5E5Qn3G3mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "b504c18f-811e-4c65-e49d-01b61a95fbb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m \u001b[32m999,992/1,000,000 \u001b[0m [ \u001b[33m0:37:28\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m408 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,992/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:37:28</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">408 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Evaluating Set_8_Large_Batch_Long_Epsilon...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 14.00\n",
            "Episode 2 finished with reward: 11.00\n",
            "Episode 3 finished with reward: 3.00\n",
            "Episode 4 finished with reward: 1.00\n",
            "Episode 5 finished with reward: 1.00\n",
            "--> Average Reward over 5 episodes: 6.00\n",
            "\n",
            "ğŸ’¾ Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_8_Large_Batch_Long_Epsilon...\n",
            "ğŸ“ Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 4 (Extended Exploration)"
      ],
      "metadata": {
        "id": "JrbWjs1wG6Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_9_Large_Batch_Higher_Final_Eps')"
      ],
      "metadata": {
        "id": "xo744nbcG8gT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c6fa4491-3eaf-409f-a86b-2f280d07208f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m \u001b[32m999,988/1,000,000 \u001b[0m [ \u001b[33m0:37:56\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m455 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,988/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:37:56</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">455 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Evaluating Set_9_Large_Batch_Higher_Final_Eps...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 5.00\n",
            "Episode 2 finished with reward: 12.00\n",
            "Episode 3 finished with reward: 7.00\n",
            "Episode 4 finished with reward: 5.00\n",
            "Episode 5 finished with reward: 5.00\n",
            "--> Average Reward over 5 episodes: 6.80\n",
            "\n",
            "ğŸ’¾ Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_9_Large_Batch_Higher_Final_Eps...\n",
            "ğŸ“ Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 5 (Large Batch)"
      ],
      "metadata": {
        "id": "VDWihQuKG-c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_10_Large_Batch_Faster_LR')"
      ],
      "metadata": {
        "id": "AfTnOqIfHAjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "0fe52207-55d8-4e20-d541-c129ca6beb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m \u001b[32m999,984/1,000,000 \u001b[0m [ \u001b[33m0:38:17\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m445 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,984/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:38:17</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">445 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Evaluating Set_10_Large_Batch_Faster_LR...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 19.00\n",
            "Episode 2 finished with reward: 19.00\n",
            "Episode 3 finished with reward: 1.00\n",
            "Episode 4 finished with reward: 1.00\n",
            "Episode 5 finished with reward: 0.00\n",
            "--> Average Reward over 5 episodes: 8.00\n",
            "\n",
            "ğŸ’¾ Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_10_Large_Batch_Faster_LR...\n",
            "ğŸ“ Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. ğŸ† Final Selection and Play Script Setup\n",
        "This cell determines the best model path and sets up the final play.py environment to use it."
      ],
      "metadata": {
        "id": "0TmnqENeSoa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. AUTOMATED FINAL EVALUATION & PLAY\n",
        "\n",
        "import gymnasium as gym\n",
        "import json\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "# --- 1. AUTOMATICALLY LOAD BEST MODEL ---\n",
        "if not os.path.exists(RESULTS_FILE):\n",
        "    print(f\"âŒ Error: Results file not found at {RESULTS_FILE}\")\n",
        "    print(\"Did you run the experiments successfully?\")\n",
        "else:\n",
        "    with open(RESULTS_FILE, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if not data:\n",
        "        print(\"âŒ Error: Results file is empty.\")\n",
        "    else:\n",
        "        # Find the key with the highest 'avg_reward'\n",
        "        best_set_name = max(data, key=lambda k: data[k]['avg_reward'])\n",
        "        best_reward = data[best_set_name]['avg_reward']\n",
        "        drive_model_path = data[best_set_name]['model_path']\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\"ğŸ† AUTOMATIC WINNER: {best_set_name}\")\n",
        "        print(f\"â­ Average Reward: {best_reward:.2f}\")\n",
        "        print(f\"ğŸ“‚ Loading Model from: {drive_model_path}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Copy to local for playing\n",
        "        final_path = \"dqn_final_best_model.zip\"\n",
        "        !cp \"$drive_model_path\" \"$final_path\"\n",
        "\n",
        "        # --- 2. PLAY SCRIPT ---\n",
        "        def make_env_render():\n",
        "            env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        def play_agent(model_path):\n",
        "            print(f\"\\nğŸ¥ Starting Agent Evaluation Video Generation...\")\n",
        "            try:\n",
        "                model = DQN.load(model_path)\n",
        "            except:\n",
        "                print(\"Model file not found. Cannot play.\")\n",
        "                return\n",
        "\n",
        "            eval_env = DummyVecEnv([make_env_render])\n",
        "            eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "            for i in range(3):\n",
        "                obs = eval_env.reset()\n",
        "                done = False\n",
        "                total_reward = 0\n",
        "                while not done:\n",
        "                    action, _ = model.predict(obs, deterministic=True)\n",
        "                    obs, reward, done_array, info = eval_env.step(action)\n",
        "                    eval_env.render() # Needed for internal state updates\n",
        "                    total_reward += reward[0]\n",
        "                    done = done_array[0]\n",
        "                print(f\"Visual Episode {i+1} finished with reward: {total_reward:.2f}\")\n",
        "            eval_env.close()\n",
        "            print(\"\\nâœ… READY!\")\n",
        "\n",
        "        play_agent(final_path)"
      ],
      "metadata": {
        "id": "uUxyQt1HSlDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15efa6df-1e6c-4257-aedd-c85546cef981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ† AUTOMATIC WINNER: Set_8_Large_Batch_Long_Epsilon\n",
            "â­ Average Reward: 8.60\n",
            "ğŸ“‚ Loading Model from: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_8_Large_Batch_Long_Epsilon/Set_8_Large_Batch_Long_Epsilon.zip\n",
            "============================================================\n",
            "\n",
            "ğŸ¥ Starting Agent Evaluation Video Generation...\n",
            "Visual Episode 1 finished with reward: 2.00\n",
            "Visual Episode 2 finished with reward: 8.00\n",
            "Visual Episode 3 finished with reward: 7.00\n",
            "\n",
            "âœ… READY!\n"
          ]
        }
      ]
    }
  ]
}