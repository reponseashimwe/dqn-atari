{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reponseashimwe/dqn-atari/blob/main/FM3_Atari_Environments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. \u2699\ufe0f Installation & Imports\n",
        "Run this cell first. It installs all dependencies and handles the necessary environment registration."
      ],
      "metadata": {
        "id": "JIsl5EEWRdAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ab7u-6RUl2",
        "outputId": "99649393-6da4-42c9-c1df-60f5ce483108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Running core imports.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# 1. INSTALLATION, REGISTRATION, AND IMPORTS\n",
        "\n",
        "# Install core DRL libraries\n",
        "!pip install stable-baselines3[extra] gymnasium[atari] ale-py -q\n",
        "# Install OpenCV and AutoROM (required dependencies for Atari wrappers)\n",
        "!pip install opencv-python autorom[accept-rom-license] -q\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import os\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import json\n",
        "\n",
        "# FIX: Explicitly register the ALE environments to avoid NamespaceNotFound\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "print(\"Setup complete. Running core imports.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. \ud83d\udcc1 Drive Setup and Experiment Definitions\n",
        "This cells handles mounting the drive and defines the five hyperparameter sets, including the one that achieved your best result (Set 1)."
      ],
      "metadata": {
        "id": "OKaFhcQmRp4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive Mount\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Drive Configuration ---\n",
        "# Mount Drive first\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive'\n",
        "ASSIGNMENT_FOLDER = 'DQN_Breakout_Final_Submission'\n",
        "BASE_DEST_PATH = f'{DRIVE_PATH}/{ASSIGNMENT_FOLDER}/Experiments'\n",
        "os.makedirs(BASE_DEST_PATH, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6m4NoZGRiV9",
        "outputId": "08a82f37-cbe8-4e4a-953e-35474abbc46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global DRL Configuration ---\n",
        "ENV_ID = \"BreakoutNoFrameskip-v4\"\n",
        "TOTAL_TIMESTEPS = 1_000_000\n",
        "N_ENVS = 4\n",
        "TEMP_MODEL_DIR = \"./temp_models/\"\n",
        "os.makedirs(TEMP_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "RESULTS_FILE = f'{BASE_DEST_PATH}/experiment_results.json'\n",
        "\n",
        "# Dictionary to store results for final comparison\n",
        "results_comparison = defaultdict(lambda: {'avg_reward': 0, 'params': {}})\n",
        "\n",
        "# --- Hyperparameter Definitions (5 Sets) ---\n",
        "HYPER_SETS = {\n",
        "    # Set 1: Baseline\n",
        "    'Set_1_Best_Policy': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 32,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    },\n",
        "    # Set 2: High LR\n",
        "    'Set_2_High_LR': {\n",
        "        'lr': 5e-4, 'gamma': 0.99, 'batch_size': 32,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    },\n",
        "    # Set 3: Low Gamma\n",
        "    'Set_3_Low_Gamma': {\n",
        "        'lr': 1e-4, 'gamma': 0.90, 'batch_size': 32,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    },\n",
        "    # Set 4: Extended Exploration\n",
        "    'Set_4_Extended_Eps': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 32,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.5\n",
        "    },\n",
        "    # Set 5: Larger Batch Size (New experiment for robust tuning)\n",
        "    'Set_5_Large_Batch': {\n",
        "        'lr': 1e-4, 'gamma': 0.99, 'batch_size': 128,\n",
        "        'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1\n",
        "    }\n",
        "}\n",
        "print(\"Experiment definitions complete. Starting training runs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtbzPHshR6iy",
        "outputId": "0fd9b9ce-413f-4077-b6ef-90677dbc0abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment definitions complete. Starting training runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. \ud83e\uddea Training, Evaluation, and Drive Saving Loop (Core Logic)\n",
        "This cell defines the functions and runs all five experiments, saving each to a distinct Drive folder."
      ],
      "metadata": {
        "id": "_wEntRpVSJGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment(env_id, n_envs):\n",
        "    env = make_atari_env(env_id, n_envs=n_envs, seed=np.random.randint(1000))\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent(model, env_id, n_episodes=5):\n",
        "    \"\"\"Runs evaluation episodes, PRINTS individual results, and returns average.\"\"\"\n",
        "    eval_env = setup_environment(env_id, n_envs=1)\n",
        "    episode_rewards = []\n",
        "\n",
        "    print(f\"\\nStarting evaluation ({n_episodes} episodes)...\")\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        obs = eval_env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done_array, info = eval_env.step(action)\n",
        "            total_reward += reward[0]\n",
        "            done = done_array[0]\n",
        "        episode_rewards.append(total_reward)\n",
        "        print(f\"Episode {i+1} finished with reward: {total_reward:.2f}\")\n",
        "\n",
        "    eval_env.close()\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    print(f\"--> Average Reward over {n_episodes} episodes: {mean_reward:.2f}\")\n",
        "    return mean_reward\n",
        "\n",
        "def save_result_to_drive(name, reward, model_path):\n",
        "    \"\"\"Reads the existing JSON on Drive, updates it, and saves it back.\"\"\"\n",
        "    data = {}\n",
        "    # 1. Load existing data if file exists\n",
        "    if os.path.exists(RESULTS_FILE):\n",
        "        try:\n",
        "            with open(RESULTS_FILE, 'r') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            data = {} # Start fresh if file is corrupt\n",
        "\n",
        "    reward_value = float(reward)\n",
        "\n",
        "    # 2. Update with new result\n",
        "    data[name] = {\n",
        "        'avg_reward': reward_value,\n",
        "        'model_path': model_path\n",
        "    }\n",
        "\n",
        "    # 3. Write back to Drive\n",
        "    with open(RESULTS_FILE, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "    print(f\"\ud83d\udcdd Result saved to persistent log: {RESULTS_FILE}\")\n",
        "\n",
        "def run_single_experiment(set_name):\n",
        "    \"\"\"Runs ONE experiment based on the set name key.\"\"\"\n",
        "    hyperparams = HYPER_SETS[set_name]\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"\ud83d\ude80 STARTING EXPERIMENT: {set_name}\")\n",
        "    print(f\"Parameters: {hyperparams}\")\n",
        "    print(f\"=\"*60)\n",
        "\n",
        "    # Paths\n",
        "    LOG_DIR = os.path.join(TEMP_MODEL_DIR, f\"{set_name}_logs\")\n",
        "    TEMP_MODEL_PATH = os.path.join(TEMP_MODEL_DIR, f\"{set_name}.zip\")\n",
        "    DRIVE_DEST_FOLDER = os.path.join(BASE_DEST_PATH, set_name)\n",
        "    os.makedirs(DRIVE_DEST_FOLDER, exist_ok=True)\n",
        "\n",
        "    # Init Environment & Model\n",
        "    env = setup_environment(ENV_ID, N_ENVS)\n",
        "\n",
        "    # Callback\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq=200000 // N_ENVS, save_path=LOG_DIR, name_prefix=\"dqn_chkpt\"\n",
        "    )\n",
        "\n",
        "    model = DQN(\n",
        "        \"CnnPolicy\", env,\n",
        "        learning_rate=hyperparams['lr'], gamma=hyperparams['gamma'],\n",
        "        batch_size=hyperparams['batch_size'],\n",
        "        exploration_initial_eps=hyperparams['epsilon_start'],\n",
        "        exploration_final_eps=hyperparams['epsilon_end'],\n",
        "        exploration_fraction=hyperparams['epsilon_decay'],\n",
        "        buffer_size=50000,\n",
        "        train_freq=4,\n",
        "        verbose=0,\n",
        "        tensorboard_log=LOG_DIR\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\u23f3 Training started... (Wait for progress bar)\")\n",
        "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=checkpoint_callback, progress_bar=True)\n",
        "    model.save(TEMP_MODEL_PATH)\n",
        "\n",
        "    # Evaluate\n",
        "    print(f\"\\n\ud83d\udcca Evaluating {set_name}...\")\n",
        "    avg_reward = evaluate_agent(model, ENV_ID, n_episodes=5)\n",
        "\n",
        "    # Save Artifacts to Drive\n",
        "    print(f\"\\n\ud83d\udcbe Copying artifacts to Drive Folder: {DRIVE_DEST_FOLDER}...\")\n",
        "    !cp -r \"$LOG_DIR\" \"$DRIVE_DEST_FOLDER/\"\n",
        "    !cp \"$TEMP_MODEL_PATH\" \"$DRIVE_DEST_FOLDER/\"\n",
        "\n",
        "    # --- AUTOMATION STEP: Save Score to JSON ---\n",
        "    final_drive_model_path = os.path.join(DRIVE_DEST_FOLDER, f\"{set_name}.zip\")\n",
        "    save_result_to_drive(set_name, avg_reward, final_drive_model_path)\n",
        "\n",
        "    # Cleanup\n",
        "    env.close()\n",
        "    del model\n",
        "    del env\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "setVfz02SOPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\u25b6\ufe0f Execution Cells (Run One by One)"
      ],
      "metadata": {
        "id": "G74m3BV2GlvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Set 1 (Best Policy)"
      ],
      "metadata": {
        "id": "2agdb-PrGowQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_1_Best_Policy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502,
          "referenced_widgets": [
            "44cb4d0144284463a2cf49a56627f4ef",
            "bcb16151a4f94e6ea1fba0eff4e3098c"
          ]
        },
        "id": "srO3lFHDSY3f",
        "outputId": "a87870e3-e550-45f4-c541-b2930c535e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "\ud83d\ude80 STARTING EXPERIMENT: Set_1_Best_Policy\n",
            "Parameters: {'lr': 0.0001, 'gamma': 0.99, 'batch_size': 32, 'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.1}\n",
            "============================================================\n",
            "\u23f3 Training started... (Wait for progress bar)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44cb4d0144284463a2cf49a56627f4ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n",
              "Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n",
              "  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: \n",
              "Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n",
              "  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Evaluating Set_1_Best_Policy...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 4.00\n",
            "Episode 2 finished with reward: 5.00\n",
            "Episode 3 finished with reward: 8.00\n",
            "Episode 4 finished with reward: 2.00\n",
            "Episode 5 finished with reward: 8.00\n",
            "--> Average Reward over 5 episodes: 5.40\n",
            "\n",
            "\ud83d\udcbe Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_1_Best_Policy...\n",
            "\ud83d\udcdd Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 2 (High LR)"
      ],
      "metadata": {
        "id": "0-J0LNs3Gvtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_2_High_LR')"
      ],
      "metadata": {
        "id": "u9X-fIr8Gy4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "3aa91349-8ca0-4113-9760-0a1a9152eae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[38;2;249;38;114m\u2578\u001b[0m \u001b[32m999,980/1,000,000 \u001b[0m [ \u001b[33m0:38:32\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m449 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,980/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:38:32</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">449 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Evaluating Set_2_High_LR...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 0.00\n",
            "Episode 2 finished with reward: 0.00\n",
            "Episode 3 finished with reward: 0.00\n",
            "Episode 4 finished with reward: 0.00\n",
            "Episode 5 finished with reward: 0.00\n",
            "--> Average Reward over 5 episodes: 0.00\n",
            "\n",
            "\ud83d\udcbe Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_2_High_LR...\n",
            "\ud83d\udcdd Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 3 (Low Gamma)"
      ],
      "metadata": {
        "id": "IAi9OlL_Gzfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_3_Low_Gamma')"
      ],
      "metadata": {
        "id": "kJ5E5Qn3G3mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "27e8c9f8-3ef1-4f3a-c3ec-0eafd498f153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[38;2;249;38;114m\u2578\u001b[0m \u001b[32m999,960/1,000,000 \u001b[0m [ \u001b[33m0:35:36\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m461 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,960/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:35:36</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">461 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Evaluating Set_3_Low_Gamma...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 3.00\n",
            "Episode 2 finished with reward: 1.00\n",
            "Episode 3 finished with reward: 1.00\n",
            "Episode 4 finished with reward: 1.00\n",
            "Episode 5 finished with reward: 1.00\n",
            "--> Average Reward over 5 episodes: 1.40\n",
            "\n",
            "\ud83d\udcbe Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_3_Low_Gamma...\n",
            "\ud83d\udcdd Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 4 (Extended Exploration)"
      ],
      "metadata": {
        "id": "JrbWjs1wG6Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_4_Extended_Eps')"
      ],
      "metadata": {
        "id": "xo744nbcG8gT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "52ee6dc6-0cc8-49f5-cf9d-72534569824f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Evaluating Set_4_Extended_Eps...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 6.00\n",
            "Episode 2 finished with reward: 2.00\n",
            "Episode 3 finished with reward: 5.00\n",
            "Episode 4 finished with reward: 1.00\n",
            "Episode 5 finished with reward: 1.00\n",
            "--> Average Reward over 5 episodes: 3.00\n",
            "\n",
            "\ud83d\udcbe Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_4_Extended_Eps...\n",
            "\ud83d\udcdd Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Set 5 (Large Batch)"
      ],
      "metadata": {
        "id": "VDWihQuKG-c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_single_experiment('Set_5_Large_Batch')"
      ],
      "metadata": {
        "id": "AfTnOqIfHAjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "f89bbaf6-5f75-4d08-ca59-93328a9dcf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[38;2;249;38;114m\u2578\u001b[0m \u001b[32m999,988/1,000,000 \u001b[0m [ \u001b[33m0:42:14\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m395 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578</span> <span style=\"color: #008000; text-decoration-color: #008000\">999,988/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:42:14</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">395 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcca Evaluating Set_5_Large_Batch...\n",
            "\n",
            "Starting evaluation (5 episodes)...\n",
            "Episode 1 finished with reward: 4.00\n",
            "Episode 2 finished with reward: 12.00\n",
            "Episode 3 finished with reward: 13.00\n",
            "Episode 4 finished with reward: 4.00\n",
            "Episode 5 finished with reward: 2.00\n",
            "--> Average Reward over 5 episodes: 7.00\n",
            "\n",
            "\ud83d\udcbe Copying artifacts to Drive Folder: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_5_Large_Batch...\n",
            "\ud83d\udcdd Result saved to persistent log: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/experiment_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. \ud83c\udfc6 Final Selection and Play Script Setup\n",
        "This cell determines the best model path and sets up the final play.py environment to use it."
      ],
      "metadata": {
        "id": "0TmnqENeSoa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. AUTOMATED FINAL EVALUATION & PLAY\n",
        "\n",
        "import gymnasium as gym\n",
        "import json\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "\n",
        "# --- 1. AUTOMATICALLY LOAD BEST MODEL ---\n",
        "if not os.path.exists(RESULTS_FILE):\n",
        "    print(f\"\u274c Error: Results file not found at {RESULTS_FILE}\")\n",
        "    print(\"Did you run the experiments successfully?\")\n",
        "else:\n",
        "    with open(RESULTS_FILE, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if not data:\n",
        "        print(\"\u274c Error: Results file is empty.\")\n",
        "    else:\n",
        "        # Find the key with the highest 'avg_reward'\n",
        "        best_set_name = max(data, key=lambda k: data[k]['avg_reward'])\n",
        "        best_reward = data[best_set_name]['avg_reward']\n",
        "        drive_model_path = data[best_set_name]['model_path']\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\"\ud83c\udfc6 AUTOMATIC WINNER: {best_set_name}\")\n",
        "        print(f\"\u2b50 Average Reward: {best_reward:.2f}\")\n",
        "        print(f\"\ud83d\udcc2 Loading Model from: {drive_model_path}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Copy to local for playing\n",
        "        final_path = \"dqn_final_best_model.zip\"\n",
        "        !cp \"$drive_model_path\" \"$final_path\"\n",
        "\n",
        "        # --- 2. PLAY SCRIPT ---\n",
        "        def make_env_render():\n",
        "            env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "            env = AtariWrapper(env)\n",
        "            return env\n",
        "\n",
        "        def play_agent(model_path):\n",
        "            print(f\"\\n\ud83c\udfa5 Starting Agent Evaluation Video Generation...\")\n",
        "            try:\n",
        "                model = DQN.load(model_path)\n",
        "            except:\n",
        "                print(\"Model file not found. Cannot play.\")\n",
        "                return\n",
        "\n",
        "            eval_env = DummyVecEnv([make_env_render])\n",
        "            eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "            for i in range(3):\n",
        "                obs = eval_env.reset()\n",
        "                done = False\n",
        "                total_reward = 0\n",
        "                while not done:\n",
        "                    action, _ = model.predict(obs, deterministic=True)\n",
        "                    obs, reward, done_array, info = eval_env.step(action)\n",
        "                    eval_env.render() # Needed for internal state updates\n",
        "                    total_reward += reward[0]\n",
        "                    done = done_array[0]\n",
        "                print(f\"Visual Episode {i+1} finished with reward: {total_reward:.2f}\")\n",
        "            eval_env.close()\n",
        "            print(\"\\n\u2705 READY!\")\n",
        "\n",
        "        play_agent(final_path)"
      ],
      "metadata": {
        "id": "uUxyQt1HSlDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0904ef-cfec-4e61-c32b-06168183ab0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "\ud83c\udfc6 AUTOMATIC WINNER: Set_5_Large_Batch\n",
            "\u2b50 Average Reward: 7.00\n",
            "\ud83d\udcc2 Loading Model from: /content/drive/MyDrive/DQN_Breakout_Final_Submission/Experiments/Set_5_Large_Batch/Set_5_Large_Batch.zip\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfa5 Starting Agent Evaluation Video Generation...\n",
            "Visual Episode 1 finished with reward: 3.00\n",
            "Visual Episode 2 finished with reward: 13.00\n",
            "Visual Episode 3 finished with reward: 5.00\n",
            "\n",
            "\u2705 READY!\n"
          ]
        }
      ]
    }
  ]
}